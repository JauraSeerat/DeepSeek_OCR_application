{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13462020,"sourceType":"datasetVersion","datasetId":8545114},{"sourceId":13462188,"sourceType":"datasetVersion","datasetId":8545095},{"sourceId":13660223,"sourceType":"datasetVersion","datasetId":8685010}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# STEP 1: Upload file (must be its own cell)\nimport platform, torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-09T22:03:23.025007Z","iopub.execute_input":"2025-11-09T22:03:23.025672Z","iopub.status.idle":"2025-11-09T22:03:26.975669Z","shell.execute_reply.started":"2025-11-09T22:03:23.025649Z","shell.execute_reply":"2025-11-09T22:03:26.975054Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%pip -q install \"transformers==4.46.3\" \"tokenizers==0.20.3\" einops addict easydict pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T22:03:26.976548Z","iopub.execute_input":"2025-11-09T22:03:26.976973Z","iopub.status.idle":"2025-11-09T22:03:41.968023Z","shell.execute_reply.started":"2025-11-09T22:03:26.976955Z","shell.execute_reply":"2025-11-09T22:03:41.967067Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ONE-CELL: pick latest uploaded image automatically → run DeepSeek-OCR\n\n\nimport os, glob, time, torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nassert torch.cuda.is_available(), \"GPU is OFF. Runtime → Change runtime type → GPU, then rerun.\"\n\n# 1) Find most recent image in /content if FNAME is missing\ndef pick_latest_image():\n    pats = [\"/content/*.png\", \"/content/*.jpg\", \"/content/*.jpeg\", \"/content/*.webp\"]\n    files = []\n    for p in pats:\n        files += glob.glob(p)\n    assert files, \"No image file found in /content. If you just uploaded, it’s saved in /content; try again.\"\n    files.sort(key=lambda f: os.path.getmtime(f), reverse=True)\n    return files[0]\n\nimg_path = \"/kaggle/input/test-img/test.png\"\n\nif not img_path:\n    img_path = pick_latest_image()\n\nprint(\"Using image:\", img_path)\n\n# 2) Optional: shrink giant screenshots for speed\nimg = Image.open(img_path).convert(\"RGB\")\nif max(img.size) > 2000:\n    s = 2000 / max(img.size)\n    img = img.resize((int(img.width*s), int(img.height*s)))\n    img_path_proc = \"/content/_shrunk.png\"\n    img.save(img_path_proc, optimize=True)\nelse:\n    img_path_proc = img_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:11:20.355589Z","iopub.execute_input":"2025-11-08T19:11:20.355926Z","iopub.status.idle":"2025-11-08T19:11:23.206095Z","shell.execute_reply.started":"2025-11-08T19:11:20.355899Z","shell.execute_reply":"2025-11-08T19:11:23.205475Z"}},"outputs":[{"name":"stdout","text":"Using image: /kaggle/input/test-img/test.png\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# 3) Load model (needs eager attention)\nmodel_id = 'deepseek-ai/DeepSeek-OCR'\ntok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nif tok.pad_token is None and tok.eos_token is not None:\n    tok.pad_token = tok.eos_token\n\nt0 = time.time()\nmodel = AutoModel.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    use_safetensors=True,\n    attn_implementation=\"eager\"   # required for this arch\n).to(dtype=torch.bfloat16, device=\"cuda\").eval()\nprint(f\"Model loaded in {time.time()-t0:.1f}s\")\n# model = model.eval().cuda().to(torch.bfloat16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:11:36.906856Z","iopub.execute_input":"2025-11-08T19:11:36.907489Z","iopub.status.idle":"2025-11-08T19:13:07.308639Z","shell.execute_reply.started":"2025-11-08T19:11:36.907466Z","shell.execute_reply":"2025-11-08T19:13:07.307732Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a65fc204db4712aa1164d39efc244e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca291e62ed4848baaa8eadad0a9a556d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef5063c55a343ad997017904e216979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b27fdcc6e34fb39c6060dc346536cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekocr.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16cb807cb41e4379bed6edc3c48e0346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conversation.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f12fea6dc4fb4a2ea592604104488a38"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekv2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe3036fb99d42139bca7c188b840e87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"328bf738a4dd42a7a5f48fc9e6bc193c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- configuration_deepseek_v2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- modeling_deepseekv2.py\n- configuration_deepseek_v2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"deepencoder.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bd70ae63ae84a58a619ad32f9f2fd99"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- modeling_deepseekocr.py\n- conversation.py\n- modeling_deepseekv2.py\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n2025-11-08 19:11:43.547047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762629103.764750      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762629103.825538      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9b151419b84b919e064a9544e6ab51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c03e34535d14633937708357cfe27c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b153a04afbcc44cb879c568cc8f26c7d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded in 89.3s\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n\n\n# 4) Fast OCR for screenshots (no tiling)\nprompt = \"<image>\\nFree OCR.\"\noutdir = \"/kaggle/working/\"; os.makedirs(outdir, exist_ok=True)\n\n@torch.inference_mode()\ndef run_ocr(path):\n    t = time.time()\n    res = model.infer(\n        tok,\n        prompt=prompt,\n        image_file=path,\n        output_path=outdir,\n        base_size=768,       # smaller canvas = faster\n        image_size=512,      # smaller tiles\n        crop_mode=False,     # screenshots usually don't need tiling\n        save_results=True,\n        test_compress=False\n    )\n    print(f\"[OK] {os.path.basename(path)} in {time.time()-t:.1f}s\")\n    return res\n\nout = run_ocr(img_path_proc)\nprint(\"Saved files in:\", outdir)\nprint(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:13:07.309968Z","iopub.execute_input":"2025-11-08T19:13:07.310494Z","iopub.status.idle":"2025-11-08T19:13:16.083205Z","shell.execute_reply.started":"2025-11-08T19:13:07.310472Z","shell.execute_reply":"2025-11-08T19:13:16.082380Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"directly resize\n","output_type":"stream"},{"name":"stderr","text":"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\nThe attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 64, 1280])\nNO PATCHES\n=====================\n# Welcome to Borrowwell\n\nWe're having trouble finding your credit score...\n\nWe couldn't match the information you provided with a profile in Equifax Canada's records.\n\n## What you can do:\n\n1. **Check your Borrowwell user profile**  \n   Make sure all information (name, date of birth, and address) is correct and up-to-date. If you need assistance in making these changes please reach out to us at [info@borwell.com](mailto:info@borwell.com).\n\n2. **Contact Equifax Canada**  \n   Confirm if your Borrowwell profile information matches your Equifax Canada file and for any active alerts. If you are unsure about what information Equifax Canada has on you, please contact them directly at 1-866-829-5961 to verify.\n\n3. **Contact Borrowwell**  \n   Once all information is correct and matches your Equifax Canada file, please contact us at [info@borwell.com](mailto:info@borwell.com) to have your identity verification questions reset.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"image: 0it [00:00, ?it/s]\nother: 0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[OK] _shrunk.png in 8.8s\nSaved files in: /kaggle/working/\nNone\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import display\n\ntext_input = widgets.Text(value='', description='Enter text:')\ndisplay(text_input)\n\ndef on_button_click(b):\n    print(f\"Input submitted: {text_input.value}\")\n\nbutton = widgets.Button(description=\"Submit\")\nbutton.on_click(on_button_click)\ndisplay(button)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T19:26:13.171668Z","iopub.execute_input":"2025-11-08T19:26:13.172435Z","iopub.status.idle":"2025-11-08T19:26:13.183319Z","shell.execute_reply.started":"2025-11-08T19:26:13.172409Z","shell.execute_reply":"2025-11-08T19:26:13.182568Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Text(value='', description='Enter text:')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93377791395d434f82536a885bc69dda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(description='Submit', style=ButtonStyle())","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb44b17112c417d87d11743bf515395"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Load DeepSeek OCR model and tokenizer\nmodel_name = \"unsloth/DeepSeek-OCR\"\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef ocr_with_deepseek(image):\n    if image is None:\n        return \"Please upload an image.\"\n    image.save(\"temp.jpg\")  # Save temporarily for model processing\n    prompt = \"<image>\\nFree OCR.\"\n    result = model.infer(\n        tokenizer,\n        prompt=prompt,\n        image_file=\"temp.jpg\",\n        base_size=1024,\n        image_size=640,\n        crop_mode=True,\n        save_results=False,\n    )\n    return result['text']\n\n# Gradio UI Setup\niface = gr.Interface(\n    fn=ocr_with_deepseek,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=gr.Textbox(label=\"OCR Output\"),\n    title=\"DeepSeek OCR Web App\",\n    description=\"Upload an image and get text extracted by DeepSeek OCR\"\n)\n\niface.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T20:01:15.404403Z","iopub.execute_input":"2025-11-08T20:01:15.405037Z","iopub.status.idle":"2025-11-08T20:01:19.893099Z","shell.execute_reply.started":"2025-11-08T20:01:15.405012Z","shell.execute_reply":"2025-11-08T20:01:19.892064Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"The repository for unsloth/DeepSeek-OCR contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/unsloth/DeepSeek-OCR.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  N\n"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_39/3410633369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load DeepSeek OCR model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"unsloth/DeepSeek-OCR\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m         trust_remote_code = resolve_trust_remote_code(\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_local_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_remote_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mresolve_trust_remote_code\u001b[0;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_remote_code\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_local_code\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    679\u001b[0m             \u001b[0;34mf\"Loading {model_name} requires you to execute the configuration file in that\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;34m\" repo on your local machine. Make sure you have read the code there to avoid malicious use, then\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Loading unsloth/DeepSeek-OCR requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."],"ename":"ValueError","evalue":"Loading unsloth/DeepSeek-OCR requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import gradio as gr\n\ndef greet(name):\n    return \"Hello \" + name + \"!\"\n\ndemo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\ndemo.launch()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T22:01:58.865740Z","iopub.execute_input":"2025-11-09T22:01:58.865981Z","iopub.status.idle":"2025-11-09T22:02:04.751823Z","shell.execute_reply.started":"2025-11-09T22:01:58.865963Z","shell.execute_reply":"2025-11-09T22:02:04.751115Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://151b6c146fff993fc2.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://151b6c146fff993fc2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import gradio as gr\nfrom transformers import AutoTokenizer, AutoModel\nimport torch, time, os\n\nmodel_id = 'deepseek-ai/DeepSeek-OCR'\ntok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nif tok.pad_token is None and tok.eos_token is not None:\n    tok.pad_token = tok.eos_token\n\nmodel = AutoModel.from_pretrained(model_id, trust_remote_code=True,\n                                  use_safetensors=True,\n                                  attn_implementation=\"eager\"\n                                 ).to(dtype=torch.bfloat16, device=\"cuda\").eval()\n\nprompt = \"<image>\\nFree OCR.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T22:56:33.003807Z","iopub.execute_input":"2025-11-09T22:56:33.004090Z","iopub.status.idle":"2025-11-09T22:56:53.462182Z","shell.execute_reply.started":"2025-11-09T22:56:33.004069Z","shell.execute_reply":"2025-11-09T22:56:53.461433Z"}},"outputs":[{"name":"stderr","text":"You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\nSome weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7862\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://d68792b1e6f7e5d851.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://d68792b1e6f7e5d851.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"directly resize\n","output_type":"stream"},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nThe `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\nThe attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n","output_type":"stream"},{"name":"stdout","text":"=====================\nBASE:  torch.Size([1, 64, 1280])\nNO PATCHES\n=====================\n# Welcome to Borrowwell\n\nWe're having trouble finding your credit score...\n\nWe couldn't match the information you provided with a profile in Equifax Canada's records.\n\n## What you can do:\n\n1. **Check your Borrowwell user profile**  \n   Make sure all information (name, date of birth, and address) is correct and up-to-date. If you need assistance in making these changes please reach out to us at [info@borwell.com](mailto:info@borwell.com).\n\n2. **Contact Equifax Canada**  \n   Confirm if your Borrowwell profile information matches your Equifax Canada file and for any active alerts. If you are unsure about what information Equifax Canada has on you, please contact them directly at 1-866-829-5961 to verify.\n\n3. **Contact Borrowwell**  \n   Once all information is correct and matches your Equifax Canada file, please contact us at [info@borwell.com](mailto:info@borwell.com) to have your identity verification questions reset.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"image: 0it [00:00, ?it/s]\nother: 0it [00:00, ?it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:18:11.541581Z","iopub.execute_input":"2025-11-09T23:18:11.542017Z","iopub.status.idle":"2025-11-09T23:18:11.545454Z","shell.execute_reply.started":"2025-11-09T23:18:11.541992Z","shell.execute_reply":"2025-11-09T23:18:11.544626Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"outdir = \"/kaggle/working/test\"; \n# os.makedirs(outdir, exist_ok=True)\n# outdir = \"./results\"\nos.makedirs(outdir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:28:16.307845Z","iopub.execute_input":"2025-11-09T23:28:16.308202Z","iopub.status.idle":"2025-11-09T23:28:16.312803Z","shell.execute_reply.started":"2025-11-09T23:28:16.308179Z","shell.execute_reply":"2025-11-09T23:28:16.311824Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"@torch.inference_mode()\ndef run_ocr(image):\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n    # image_path = \"/kaggle/input/test-img/test.png\" \n    # Define Kaggle input directory\n    input_dir = \"/kaggle/working/\"\n    os.makedirs(input_dir, exist_ok=True)  # Ensure directory exists\n\n    # Generate a unique filename (to avoid overwriting files)\n    image_filename = f\"test_21.png\"\n    image_path = os.path.join(input_dir, image_filename)\n\n    # Save uploaded image to the Kaggle input directory\n    image.save(image_path)\n    # image.save(image_path)\n    res = model.infer(tok, prompt=prompt, image_file=image_path,\n                      output_path=outdir, base_size=768,\n                      image_size=512, crop_mode=False, save_results=True)\n    # print(type(res))\n    # Path to results.mmd\n    text_file = os.path.join(outdir, \"results.mmd\")\n    \n    if os.path.exists(\"/kaggle/working/test/result.mmd\"):\n        with open(\"/kaggle/working/test/result.mmd\", \"r\") as f:\n            ocr_text = f.read()\n    else:\n        ocr_text = \"OCR completed, but text file not found.\"\n    return ocr_text\n\n\n\n\nui = gr.Interface(fn=run_ocr, inputs=\"image\", outputs=\"text\",\n                  title=\"DeepSeek OCR\", description=\"Upload an image to extract text.\")\nui.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:49:05.132973Z","iopub.execute_input":"2025-11-09T23:49:05.133232Z","iopub.status.idle":"2025-11-09T23:49:06.106763Z","shell.execute_reply.started":"2025-11-09T23:49:05.133212Z","shell.execute_reply":"2025-11-09T23:49:06.106179Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7874\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://558954394df0529bbf.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://558954394df0529bbf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"directly resize\n=====================\nBASE:  torch.Size([1, 64, 1280])\nNO PATCHES\n=====================\n# Welcome to Borrowwell\n\nWe're having trouble finding your credit score...\n\nWe couldn't match the information you provided with a profile in Equifax Canada's records.\n\n## What you can do:\n\n1. **Check your Borrowwell user profile**  \n   Make sure all information (name, date of birth, and address) is correct and up-to-date. If you need assistance in making these changes please reach out to us at [info@borwell.com](mailto:info@borwell.com).\n\n2. **Contact Equifax Canada**  \n   Confirm if your Borrowwell profile information matches your Equifax Canada file and for any active alerts. If you are unsure about what information Equifax Canada has on you, please contact them directly at 1-866-829-5961 to verify.\n\n3. **Contact Borrowwell**  \n   Once all information is correct and matches your Equifax Canada file, please contact us at [info@borwell.com](mailto:info@borwell.com) to have your identity verification questions reset.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"image: 0it [00:00, ?it/s]\nother: 0it [00:00, ?it/s]\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"directly resize\n=====================\nBASE:  torch.Size([1, 64, 1280])\nNO PATCHES\n=====================\n# We have experienced an issue processing your request\n\nWe're sorry, we are unable to process your request at this time. Please contact our Customer Care Team at 1-866-828-9361 for additional help completing your request.\n\nThe Customer Care Team is available between 9:00 AM and 9:00 PM ET, Mon-Fri; 9:00 AM and 6:00 PM ET, Sat-Sun\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"image: 0it [00:00, ?it/s]\nother: 0it [00:00, ?it/s]\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"directly resize\n=====================\nBASE:  torch.Size([1, 64, 1280])\nNO PATCHES\n=====================\n# Welcome to Borrowwell\n\nWe're having trouble finding your credit score...\n\nWe couldn't match the information you provided with a profile in Equifax Canada's records.\n\n## What you can do:\n\n1. **Check your Borrowwell user profile**  \n   Make sure all information (name, date of birth, and address) is correct and up-to-date. If you need assistance in making these changes please reach out to us at [info@borwell.com](mailto:info@borwell.com).\n\n2. **Contact Equifax Canada**  \n   Confirm if your Borrowwell profile information matches your Equifax Canada file and for any active alerts. If you are unsure about what information Equifax Canada has on you, please contact them directly at 1-866-829-5961 to verify.\n\n3. **Contact Borrowwell**  \n   Once all information is correct and matches your Equifax Canada file, please contact us at [info@borwell.com](mailto:info@borwell.com) to have your identity verification questions reset.\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"image: 0it [00:00, ?it/s]\nother: 0it [00:00, ?it/s]\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"directly resize\n=====================\nBASE:  torch.Size([1, 64, 1280])\nNO PATCHES\n=====================\n# We have experienced an issue processing your request\n\nWe're sorry, we are unable to process your request at this time. Please contact our Customer Care Team at 1-866-828-9361 for additional help completing your request.\n\nThe Customer Care Team is available between 9:00 AM and 9:00 PM ET, Mon-Fri; 9:00 AM and 6:00 PM ET, Sat-Sun\n===============save results:===============\n","output_type":"stream"},{"name":"stderr","text":"image: 0it [00:00, ?it/s]\nother: 0it [00:00, ?it/s]\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"directly resize\n=====================\nBASE:  torch.Size([1, 64, 1280])\nNO PATCHES\n=====================\n# Annual Statement of Account\n\n**BRIMSBANKAN JIJIYA**  \n**BANK OF BANGLADESH**  \n**WOMEN'S AND BOY'S TURF**  \n**Permanent Number:**  \n**SERIES NO.:**  \n**SERIAL NO.:**  \n**DATE OF ISSUE:**  \n**DATE OF END OF YEAR:**  \n**DISTRIBUTION:**  \n**DISTRIBUTION DATE:**  \n**DISTRIBUTION PERIOD:**  \n**DISTRIBUTION PERIOD DATE:**  \n**DISTRIBUTION PERIOD DATE:**  \n**DISTRIBUTION PERIOD:**  \n**DISTRIBUTION PERIOD:**  \n**DISTRIBUTION PERIOD DATE:**\n\n---\n\n## The In a Readinessment year.\n\nThe income earned in the year is not used for the next year.  \nThe income earned in the year is used for the next year.  \nMIT Service Centre under you want to make a change.\n\n---\n\n## Annual payment summary (for page 2 of detail)\n\n| Amount | $0.00 |\n|---|---|\n| Annual total | $0.00 |\n\n---\n\n## Before November 24, 2025, the following payment is due:\n\n| Category | Amount |\n|---|---|\n| Due    | $0.00  |\n\n---\n\n**Please be assured that we are after November 24, 2025, will make the payment in the next month. If you have any questions, please contact our office. We are happy to assist you with your business, and we are here to help. November 24, 2025, is a special date for us.**\n\n---\n\n## Where do your premium dollars go?\n\n**Basic Autopay**  \n**60%**  \n**Due August 2025**\n\n---\n\n## 21%  \n**Upfront**\n\n- **3%**  \n  **Premium taxes**  \n- **1%**  \n  **CITI**  \n- **4%**  \n  **CITI**  \n- **2%**  \n  **CITI**  \n- **1%**  \n  **CITI**  \n- **1%**  \n  **Citi**  \n- **1%**  \n  **Citi**  \n- **1%**  \n\n---\n\n1. Demyst Circle Vehicle collection, Comprehensive (mandatory, both job etc.).  \n2. Basic Autopay (company for certain other major claims) paid.  \n3. Basic Deposit.  \n4. Basic Deposit.  \n5. Basic Deposit.  \n6. Basic Deposit.  \n7. Basic Deposit.  \n8. Basic Deposit.  \n9. Basic Deposit.  \n10. Basic Deposit.  \n11. Basic Deposit.  \n12. Basic Deposit.  \n13. Basic Deposit.  \n14. Basic Deposit.  \n15. Basic Deposit.  \n16. Basic Deposit.  \n17. Basic Deposit.  \n18. Basic Deposit.  \n19. Basic Deposit.  \n20. Basic Deposit.  \n21. Basic Deposit.  \n22. Basic Deposit.  \n23. Basic Deposit.  \n24. Basic Deposit.  \n25. Basic Deposit.  \n26. Basic Deposit.  \n27. Basic Deposit.  \n28. Basic Deposit.  \n29. Basic Deposit.  \n30. Basic Deposit.  \n31. Basic Deposit.  \n32. Basic Deposit.  \n33. Basic Deposit.  \n34. Basic Deposit.  \n35. Basic Deposit.  \n36. Basic Deposit.  \n37. Basic Deposit.  \n38. Basic Deposit.  \n39. Basic Deposit.  \n40. Basic Deposit.  \n41. Basic Deposit.  \n42. Basic Deposit.  \n43. Basic Deposit.  \n44. Basic Deposit.  \n45. Basic Deposit.  \n46. Basic Deposit.  \n47. Basic Deposit.  \n48. Basic Deposit.  \n49. Basic Deposit.  \n50. Basic Deposit.  \n51. Basic Deposit.  \n52. Basic Deposit.  \n53. Basic Deposit.  \n54. Basic Deposit.  \n55. Basic Deposit.  \n56. Basic Deposit.  \n57. Basic Deposit.  \n58. Basic Deposit.  \n59. Basic Deposit.  \n60. Basic Deposit.  \n61. Basic Deposit.  \n62. Basic Deposit.  \n63. Basic Deposit.  \n64. Basic Deposit.  \n65. Basic Deposit.  \n66. Basic Deposit.  \n67. Basic Deposit.  \n68. Basic Deposit.  \n69. Basic Deposit.  \n70. Basic Deposit.  \n71. Basic Deposit.  \n72. Basic Deposit.  \n73. Basic Deposit.  \n74. Basic Deposit.  \n75. Basic Deposit.  \n76. Basic Deposit.  \n77. Basic Deposit.  \n78. Basic Deposit.  \n79. Basic Deposit.  \n80. Basic Deposit.  \n81. Basic Deposit.  \n82. Basic Deposit.  \n83. Basic Deposit.  \n84. Basic Deposit.  \n85. Basic Deposit.  \n86. Basic Deposit.  \n87. Basic Deposit.  \n88. Basic Deposit.  \n89. Basic Deposit.  \n90. Basic Deposit.  \n91. Basic Deposit.  \n92. Basic Deposit.  \n93. Basic Deposit.  \n94. Basic Deposit.  \n95. Basic Deposit.  \n96. Basic Deposit.  \n97. Basic Deposit.  \n98. Basic Deposit.  \n99. Basic Deposit.  \n100. Basic Deposit.  \n101. Basic Deposit.  \n102. Basic Deposit.  \n103. Basic Deposit.  \n104. Basic Deposit.  \n105. Basic Deposit.  \n106. Basic Deposit.  \n107. Basic Deposit.  \n108. Basic Deposit.  \n109. Basic Deposit.  \n110. Basic Deposit.  \n111. Basic Deposit.  \n112. Basic Deposit.  \n113. Basic Deposit.  \n114. Basic Deposit.  \n115. Basic Deposit.  \n116. Basic Deposit.  \n117. Basic Deposit.  \n118. Basic Deposit.  \n119. Basic Deposit.  \n120. Basic Deposit.  \n121. Basic Deposit.  \n122. Basic Deposit.  \n123. Basic Deposit.  \n124. Basic Deposit.  \n125. Basic Deposit.  \n126. Basic Deposit.  \n127. Basic Deposit.  \n128. Basic Deposit.  \n129. Basic Deposit.  \n130. Basic Deposit.  \n131. Basic Deposit.  \n132. Basic Deposit.  \n133. Basic Deposit.  \n134. Basic Deposit.  \n135. Basic Deposit.  \n136. Basic Deposit.  \n137. Basic Deposit.  \n138. Basic Deposit.  \n139. Basic Deposit.  \n140. Basic Deposit.  \n141. Basic Deposit.  \n142. Basic Deposit.  \n143. Basic Deposit.  \n144. Basic Deposit.  \n145. Basic Deposit.  \n146. Basic Deposit.  \n147. Basic Deposit.  \n148. Basic Deposit.  \n149. Basic Deposit.  \n150. Basic Deposit.  \n151. Basic Deposit.  \n152. Basic Deposit.  \n153. Basic Deposit.  \n154. Basic Deposit.  \n155. Basic Deposit.  \n156. Basic Deposit.  \n157. Basic Deposit.  \n158. Basic Deposit.  \n159. Basic Deposit.  \n160. Basic Deposit.  \n161. Basic Deposit.  \n162. Basic Deposit.  \n163. Basic Deposit.  \n164. Basic Deposit.  \n165. Basic Deposit.  \n166. Basic Deposit.  \n167. Basic Deposit.  \n168. Basic Deposit.  \n169. Basic Deposit.  \n170. Basic Deposit.  \n171. Basic Deposit.  \n172. Basic Deposit.  \n173. Basic Deposit.  \n174. Basic Deposit.  \n175. Basic Deposit.  \n176. Basic Deposit.  \n177. Basic Deposit.  \n178. Basic Deposit.  \n179. Basic Deposit.  \n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"with open(\"/kaggle/working/test/result.mmd\", \"r\") as f:\n        result_text = f.read()\nprint(result_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T23:48:27.120165Z","iopub.execute_input":"2025-11-09T23:48:27.120675Z","iopub.status.idle":"2025-11-09T23:48:27.125034Z","shell.execute_reply.started":"2025-11-09T23:48:27.120650Z","shell.execute_reply":"2025-11-09T23:48:27.124411Z"}},"outputs":[{"name":"stdout","text":"# Welcome to Borrowwell\n\nWe're having trouble finding your credit score...\n\nWe couldn't match the information you provided with a profile in Equifax Canada's records.\n\n## What you can do:\n\n1. **Check your Borrowwell user profile**  \n   Make sure all information (name, date of birth, and address) is correct and up-to-date. If you need assistance in making these changes please reach out to us at [info@borwell.com](mailto:info@borwell.com).\n\n2. **Contact Equifax Canada**  \n   Confirm if your Borrowwell profile information matches your Equifax Canada file and for any active alerts. If you are unsure about what information Equifax Canada has on you, please contact them directly at 1-866-829-5961 to verify.\n\n3. **Contact Borrowwell**  \n   Once all information is correct and matches your Equifax Canada file, please contact us at [info@borwell.com](mailto:info@borwell.com) to have your identity verification questions reset.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}